{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set the font size of plots\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = [ './sentiment_data_TUN_pos.txt', './sentiment_data_TUN_neg.txt','./langid_data_ARA.txt' ]\n",
    "df = pd.read_csv(\"QuestionnaireData_28Nov2018.csv\", encoding='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    print('Reading file ' + filename + \"...\")\n",
    "    with open(filename, \"r\", encoding='utf8') as textfile:\n",
    "        L = []\n",
    "        for line in textfile:\n",
    "            L.append(line.strip())\n",
    "        print('File contains ', len(L), \"lines.\\n\")\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./sentiment_data_TUN_pos.txt...\n",
      "File contains  3468 lines.\n",
      "\n",
      "Reading file ./sentiment_data_TUN_neg.txt...\n",
      "File contains  4345 lines.\n",
      "\n",
      "Reading file ./langid_data_ARA.txt...\n",
      "File contains  21787 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ara_corpus_pos = read_text_file(corpus_files[0])\n",
    "ara_corpus_neg = read_text_file(corpus_files[1])\n",
    "ara_ara_corpus = read_text_file(corpus_files[2])\n",
    "other_corpus = df.iloc[:,8].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine pos and neg corpus into a single corpus for easy manipulation\n",
    "\n",
    "ara_corpus = ara_corpus_pos + ara_corpus_neg\n",
    "ara_corpus_sentiment = len(ara_corpus_pos)*[1] + len(ara_corpus_neg)*[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7813, 7813)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ara_corpus),len(ara_corpus_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing & Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1. Convert string to emoji\n",
    "dict_emot= { ':-)'  : b'\\xf0\\x9f\\x98\\x8a'.decode('utf-8'),\n",
    "              ':)'   : b'\\xf0\\x9f\\x98\\x8a'.decode('utf-8'),\n",
    "             '=)'   : b'\\xf0\\x9f\\x98\\x8a'.decode('utf-8'),  # Smile or happy\n",
    "              ':-D'  : b'\\xf0\\x9f\\x98\\x83'.decode('utf-8'),\n",
    "              ':D'   : b'\\xf0\\x9f\\x98\\x83'.decode('utf-8'),\n",
    "             '=D'   : b'\\xf0\\x9f\\x98\\x83'.decode('utf-8'),  # Big smile\n",
    "             '>:-(' : b'\\xF0\\x9F\\x98\\xA0'.decode('utf-8'),\n",
    "              '>:-o' : b'\\xF0\\x9F\\x98\\xA0'.decode('utf-8')   # Angry face\n",
    "             }\n",
    "def string_to_emoji(string):\n",
    "    emoji_pattern = re.compile(r\"(:\\-?\\))|(:\\-?D)|=D|=\\)|(>:\\-[o\\(])\")  \n",
    "    return emoji_pattern.sub( lambda x: dict_emot[x.group()] , string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def char_is_emoji(character):\n",
    "    return character in emoji.UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تبارك الله عليه\n",
      "تبارك الله عليه\n",
      "نور شيبة فنان يعجبني صوتو حلوا و مزيان :)\n",
      "نور شيبة فنان يعجبني صوتو حلوا و مزيان 😊\n",
      "♥♥♥\n",
      "♥♥♥\n"
     ]
    }
   ],
   "source": [
    "##1. Remove useless characters using cleanup_text function from TD2\n",
    "\n",
    "# YOU CAN MODIFY THIS FUNCTION AS NEEDED.\n",
    "# FOR EXAMPLE, REMOVE NUMBERS ...\n",
    "\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "try:\n",
    "    maketrans = ''.maketrans\n",
    "except AttributeError:\n",
    "    # fallback for Python 2\n",
    "    from string import maketrans\n",
    "\n",
    "# regexp for word elongation: matches 3 or more repetitions of a word character.\n",
    "two_plus_letters_RE = re.compile(r\"(\\w)\\1{1,}\", re.DOTALL)\n",
    "three_plus_letters_RE = re.compile(r\"(\\w)\\1{2,}\", re.DOTALL)\n",
    "# regexp for repeated words\n",
    "two_plus_words_RE = re.compile(r\"(\\w+\\s+)\\1{1,}\", re.DOTALL)\n",
    "\n",
    "\n",
    "def cleanup_text(text):\n",
    "    \n",
    "    text =string_to_emoji(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "\n",
    "    # Remove user mentions of the form @username\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    \n",
    "    # Replace special html-encoded characters with their ASCII equivalent, for example: &#39 ==> '\n",
    "    if re.search(\"&#\",text):\n",
    "        text = html.unescape(text)\n",
    "\n",
    "    # Remove special useless characters such as _x000D_\n",
    "    text = re.sub(r'_[xX]000[dD]_', '', text)\n",
    "\n",
    "    # Replace all non-word characters (such as  punctuation, end of line characters, etc.) with a space\n",
    "    text = text.translate(maketrans(\"\",\"\", string.punctuation))\n",
    "\n",
    "    # Remove redundant white spaces\n",
    "    text = text.strip()\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "\n",
    "    # normalize word elongations (characters repeated more than twice)\n",
    "    text = two_plus_letters_RE.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # remove repeated words\n",
    "    text = two_plus_words_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# unit test of this function\n",
    "print(ara_corpus[0])\n",
    "print(cleanup_text(ara_corpus[0]))\n",
    "print(ara_corpus[6])\n",
    "print(cleanup_text(ara_corpus[6]))\n",
    "print(ara_corpus[19])\n",
    "print(cleanup_text(ara_corpus[19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this function to each document in the corpus\n",
    "ara_corpus_clean = []\n",
    "\n",
    "for doc in ara_corpus:\n",
    "    ara_corpus_clean.append(cleanup_text(doc))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['تبارك الله عليه', 'امرأة أنيقة و سلسة في خطابها و ذكية و متواضعة', 'جعفر يحب بلادو ويعمل في الخير يعاون في الناس والحكومة لا علاقة تسرق في الشعب وتشري في الكوش للباجي', 'والله بكتني برافووووو واصل', 'محلاها', 'والله احسن ما صار في قناة الحوار التونسي المنشط هاذا', 'نور شيبة فنان يعجبني صوتو حلوا و مزيان :)', 'احسن ممثل في ادوار الكول', 'الرجل المناسب في المكان المناسب', 'عسل']\n",
      "['تبارك الله عليه', 'امرأة أنيقة و سلسة في خطابها و ذكية و متواضعة', 'جعفر يحب بلادو ويعمل في الخير يعاون في الناس والحكومة لا علاقة تسرق في الشعب وتشري في الكوش للباجي', 'والله بكتني برافوو واصل', 'محلاها', 'والله احسن ما صار في قناة الحوار التونسي المنشط هاذا', 'نور شيبة فنان يعجبني صوتو حلوا و مزيان 😊', 'احسن ممثل في ادوار الكول', 'الرجل المناسب في المكان المناسب', 'عسل']\n"
     ]
    }
   ],
   "source": [
    "assert(len(ara_corpus_clean)==len(ara_corpus))\n",
    "print(ara_corpus[:10])\n",
    "print(ara_corpus_clean[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LAT_FRAC = 0.3\n",
    "ara_corpus_clean = [doc for doc in ara_corpus_clean if(len(doc) >0) if (len(re.findall('[a-zA-Z]',doc)) / len(doc)) < MAX_LAT_FRAC]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7813 7786\n"
     ]
    }
   ],
   "source": [
    "assert(len(ara_corpus_clean)<=len(ara_corpus))\n",
    "print(len(ara_corpus),len(ara_corpus_clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeArabic(corpus):\n",
    "    corpus = re.sub(\"ة\", \"ت\", corpus)\n",
    "    corpus = re.sub(\"ض\", \"ظ\", corpus) \n",
    "    corpus = re.sub(\"ى\", \"ي\", corpus)\n",
    "    corpus = re.sub(\"ؤ\", \"ء\", corpus)\n",
    "    corpus = re.sub(\"ئ\", \"ء\", corpus)\n",
    "    corpus = re.sub(\"[إأٱآا]\", \"ا\", corpus)\n",
    "    return(corpus)\n",
    "\n",
    "ara_corpus_clean =  [normalizeArabic(doc) for doc in ara_corpus_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[^_\\s]+')\n",
    "ara_corpus_tokenized = [tokenizer.tokenize(doc) for doc in ara_corpus_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert(len(ara_corpus_clean) == len(ara_corpus_tokenized))\n",
    "assert(type(ara_corpus_tokenized[0]) == list and type(ara_corpus_tokenized[0][0]) ==str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['تبارك', 'الله', 'عليه'],\n",
       " ['امرات', 'انيقت', 'و', 'سلست', 'في', 'خطابها', 'و', 'ذكيت', 'و', 'متواظعت'],\n",
       " ['جعفر',\n",
       "  'يحب',\n",
       "  'بلادو',\n",
       "  'ويعمل',\n",
       "  'في',\n",
       "  'الخير',\n",
       "  'يعاون',\n",
       "  'في',\n",
       "  'الناس',\n",
       "  'والحكومت',\n",
       "  'لا',\n",
       "  'علاقت',\n",
       "  'تسرق',\n",
       "  'في',\n",
       "  'الشعب',\n",
       "  'وتشري',\n",
       "  'في',\n",
       "  'الكوش',\n",
       "  'للباجي'],\n",
       " ['والله', 'بكتني', 'برافوو', 'واصل'],\n",
       " ['محلاها']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify first document in corpus\n",
    "ara_corpus_tokenized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nasri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##5. Remove stop words -- based on a 'standard' list of stopwords for the Arabic language.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Load stop words from NLTK library\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_ar = stopwords.words('arabic')\n",
    "type(stop_words_ar),len(stop_words_ar)\n",
    "set(stop_words_ar) & {'من','إلى','عن','على','في','ب','ل','ك','و'}\n",
    "stop_words_ar = stop_words_ar + ['هدا','ها','بش','من','إلى','عن','على','في','ب','ل','ك','و']\n",
    "type(stop_words_ar)\n",
    "# For each document, remove stop words\n",
    "ara_corpus_tokenized = [[word for word in doc if word not in stop_words_ar] for doc in ara_corpus_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##6. Stemming\n",
    "import argparse\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "def light_stem(words):\n",
    "    \n",
    "    result = list()\n",
    "    stemmer = ISRIStemmer()\n",
    "    for word in words:\n",
    "        word = stemmer.norm(word, num=1)      # remove diacritics which representing Arabic short vowels\n",
    "        if not word in stemmer.stop_words:    # exclude stop words from being processed\n",
    "            word = stemmer.pre32(word)        # remove length three and length two prefixes in this order\n",
    "            word = stemmer.suf32(word)        # remove length three and length two suffixes in this order\n",
    "            word = stemmer.waw(word)          # remove connective ‘و’ if it precedes a word beginning with ‘و’\n",
    "            word = stemmer.norm(word, num=2)  # normalize initial hamza to bare alif\n",
    "        result.append(word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "act=ara_corpus_tokenized\n",
    "ara_corpus_clean =  [light_stem(words) for words in ara_corpus_tokenized ]\n",
    "ara_corpus_tokenized =  [tokenizer.tokenize(doc) for doc in ara_corpus_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['تبارك', 'الله'], ['امرات', 'انيقت', 'سلست', 'خطابها', 'ذكيت', 'متواظعت'], ['جعفر', 'يحب', 'بلادو', 'ويعمل', 'الخير', 'يعاون', 'الناس', 'والحكومت', 'علاقت', 'تسرق', 'الشعب', 'وتشري', 'الكوش', 'للباجي'], ['والله', 'بكتني', 'برافوو', 'واصل'], ['محلاها'], ['والله', 'احسن', 'صار', 'قنات', 'الحوار', 'التونسي', 'المنشط', 'هاذا']]\n",
      "[['تبارك', 'الله'], ['امر', 'انيقت', 'سلست', 'خطاب', 'ذكيت', 'متواظعت'], ['جعفر', 'يحب', 'بلادو', 'ويعمل', 'خير', 'يعا', 'ناس', 'حكومت', 'علاقت', 'تسرق', 'شعب', 'وتشري', 'كوش', 'باجي'], ['والله', 'بكت', 'برافوو', 'واصل'], ['محلا'], ['والله', 'احسن', 'صار', 'قنات', 'حوار', 'تونسي', 'منشط', 'هاذا']]\n"
     ]
    }
   ],
   "source": [
    "print(act[:6])\n",
    "print(ara_corpus_tokenized[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14243.000000\n",
       "mean         4.882398\n",
       "std          1.852973\n",
       "min          1.000000\n",
       "25%          4.000000\n",
       "50%          5.000000\n",
       "75%          6.000000\n",
       "max         58.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##7. Remove words that are too short or too long.\n",
    "\n",
    "distinct_words = {word for doc in ara_corpus_tokenized for word in doc}\n",
    "type(distinct_words),len(distinct_words)\n",
    "\n",
    "words_len = pd.Series([len(word) for word in distinct_words])\n",
    "words_len.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 7786)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ara_corpus_tokenized = [[word for word in doc if len(word)>=4 or char_is_emoji(word)] for doc in ara_corpus_tokenized]\n",
    "ara_corpus_tokenized = [[word for word in doc if len(word)<=12] for doc in ara_corpus_tokenized]\n",
    "type(ara_corpus_tokenized),len(ara_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['تبارك', 'الله'], ['انيقت', 'سلست', 'خطاب', 'ذكيت', 'متواظعت'], ['جعفر', 'بلادو', 'ويعمل', 'حكومت', 'علاقت', 'تسرق', 'وتشري', 'باجي'], ['والله', 'برافوو', 'واصل'], ['محلا'], ['والله', 'احسن', 'قنات', 'حوار', 'تونسي', 'منشط', 'هاذا'], ['شيبت', 'فنان', 'يعجب', 'صوتو', 'حلوا', '😊'], ['احسن', 'ممثل', 'ادوار'], ['مناسب', 'مكان', 'مناسب'], []]\n"
     ]
    }
   ],
   "source": [
    "print(ara_corpus_tokenized[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a  language classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a language classifier using supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tun_corpus_clean = [cleanup_text(doc)   for doc in ara_corpus]\n",
    "ara_corpus_clean = [cleanup_text(doc)   for doc in ara_ara_corpus]\n",
    "\n",
    "# Step 1   COMPLETE THE CODE BELOW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#?train_test_split\n",
    "\n",
    "tun_corpus_clean_train, tun_corpus_clean_test = train_test_split(tun_corpus_clean,test_size=0.3 )\n",
    "\n",
    "ara_corpus_clean_train, ara_corpus_clean_test = train_test_split(ara_corpus_clean,test_size=0.3 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1   create 2 data frames called train_df and test_df (as explained above)\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "# create data frame\n",
    "train_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "\n",
    "# fill the language column\n",
    "train_df.language = pd.Series(['TUN']*len(tun_corpus_clean_train) + ['ARA']*len(tun_corpus_clean_train) )\n",
    "\n",
    "\n",
    "# fill the document column -- CONCATENATE the TUN CORPUS and ARA CORPUS\n",
    "train_df.document = pd.Series(tun_corpus_clean_train + ara_corpus_clean_train )\n",
    "\n",
    "test_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "test_df.language = pd.Series(['TUN']*len(tun_corpus_clean_test) + ['ARA']*len(ara_corpus_clean_test) )\n",
    "test_df.document = pd.Series(tun_corpus_clean_test + ara_corpus_clean_test   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2  Convert the training documents into numeric feature vectors using the BOW-tfidf method with character ngrams.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "n = 3   # hyperparameter for of character ngrams ; you can change it if you want but n=3 is a reaonable value ...\n",
    "\n",
    "# Create an instance of TfidfVectorizer class with analyzer = 'char' so that it generates bag of characters and not bag of words\n",
    "bow_model_char = TfidfVectorizer(analyzer='char', ngram_range=(1,n), max_df =0.9, min_df=0.1)\n",
    "\n",
    "# Call fit method with the combined training corpus\n",
    "bow_model_char.fit(train_df.document)\n",
    "\n",
    "# Create DTM matrix of the combined training corpus and test corpus\n",
    "dtm_Train=bow_model_char.transform(train_df.document)\n",
    "dtm_Test=bow_model_char.transform(test_df.document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TUN'] TUN\n",
      "Confusion matrix :  [[5997  540]\n",
      " [ 269 2075]]\n"
     ]
    }
   ],
   "source": [
    "# Step 3   -- see official documentation of MultinomialNB in scikit-learn\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(dtm_Train,train_df.language)\n",
    "\n",
    "print(nb_model.predict(dtm_Test[0]),test_df.language[0])\n",
    "\n",
    "print(\"Confusion matrix : \",confusion_matrix(test_df.language,nb_model.predict(dtm_Test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(doc):\n",
    "    dtm=bow_model_char.transform(pd.Series(doc))\n",
    "    return nb_model.predict(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splt(x):\n",
    "    u = str(x).split(':')\n",
    "    return [u[0][1:],float(u[1][0:-2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(doc):\n",
    "    lg=splt(langdetect.detect_langs(doc))\n",
    "    if lg[0]=='ar' :\n",
    "        Lang=predict(doc)\n",
    "    else:\n",
    "        Lang='[\\'OTHER\\']'\n",
    "    print(Lang)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OTHER']\n",
      "['ARA']\n",
      "['TUN']\n"
     ]
    }
   ],
   "source": [
    "lang_detect(other_corpus[0])\n",
    "lang_detect(ara_corpus_clean_test[0])\n",
    "lang_detect(tun_corpus_clean_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the corpus for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, concatenate the words in the cleaned corpus (because BOW method in scikit-learn requires this format)\n",
    "ara_corpus_bow = [' '.join(doc) for doc in ara_corpus_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Configuration parameters of the BOW model\n",
    "# FEEL FREE TO MODIFY THESE PARAMETERS AS NEEDED ...\n",
    "max_words = 10000\n",
    "maxdf = 0.7\n",
    "mindf = 5\n",
    "# create an instance of this class\n",
    "bow_model = TfidfVectorizer(max_df=maxdf, min_df=mindf, max_features=max_words, stop_words=[], use_idf = True)\n",
    "# call fit() method in order to prepare BOW method (determine vocabulary and IDF values)\n",
    "bow_model.fit( ara_corpus_bow )\n",
    "# Call the transform method in order to calculate DTM matrix of our corpus\n",
    "ara_bow_dtm = bow_model.transform(ara_corpus_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove documents that do not contain any vocabulary terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_terms_per_doc = np.array((ara_bow_dtm>0).sum(axis=1))  # calculate sum of rows of DTM matrix\n",
    "nb_terms_per_doc = nb_terms_per_doc.ravel()  # convert result to a 1D array (instead of 2D array)\n",
    "idx = nb_terms_per_doc>0\n",
    "ara_bow_dtm_filt = ara_bow_dtm[nb_terms_per_doc>0,:]\n",
    "ara_corpus_bow_filt = [ara_corpus_bow[i] for i,x in enumerate(idx) if x]\n",
    "ara_corpus_sentiment_filt = [ara_corpus_sentiment[i] for i,x in enumerate(idx) if x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Sentiment Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ara_bow_dtm_filt\n",
    "y = ara_corpus_sentiment_filt\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1076,   83],\n",
       "       [  84,  870]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build Naive Bayes classification model\n",
    "\n",
    "NB_model = MultinomialNB(alpha = 1.0)\n",
    "NB_model.fit(X_train, y_train)\n",
    "# Use this model to predict the sentiment category of test documents\n",
    "y_pred_NB = NB_model.predict(X_test)\n",
    "# Classification rate\n",
    "metrics.accuracy_score(y_test, y_pred_NB)\n",
    "# Confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classefy(doc):\n",
    "    ara_corpus_bow=[]\n",
    "    ara_corpus_bow.append(doc)\n",
    "    ara_bow_dtm = bow_model.transform(ara_corpus_bow)\n",
    "    nb_terms_per_doc = np.array((ara_bow_dtm>0).sum(axis=1))  # calculate sum of rows of DTM matrix\n",
    "    nb_terms_per_doc = nb_terms_per_doc.ravel()  # convert result to a 1D array (instead of 2D array)\n",
    "    idx = nb_terms_per_doc>0\n",
    "    ara_bow_dtm_filt = ara_bow_dtm[nb_terms_per_doc>0,:]\n",
    "    ara_corpus_bow_filt = [ara_corpus_bow[i] for i,x in enumerate(idx) if x]\n",
    "    if len(ara_corpus_bow_filt)==0:\n",
    "        print('0')\n",
    "    else:\n",
    "        y=(NB_model.predict(ara_bow_dtm_filt))\n",
    "        print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "classefy(other_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "classefy(ara_corpus_neg [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "classefy(ara_corpus_pos [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1097   62]\n",
      " [ 117  837]]\n"
     ]
    }
   ],
   "source": [
    "# Train the model using Logistic Regression method\n",
    "LR_model = LogisticRegression(penalty='l2')\n",
    "LR_model.fit(X_train, y_train)\n",
    "# Use this model to predict the sentiment category of test documents\n",
    "y_pred_LR = LR_model.predict(X_test)\n",
    "# Calculate the classification rate of this classifier\n",
    "metrics.accuracy_score(y_test, y_pred_LR)\n",
    "# Display the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB method seems better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
