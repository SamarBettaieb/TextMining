{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set the font size of plots\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = [ './sentiment_data_TUN_pos.txt', './sentiment_data_TUN_neg.txt','./langid_data_ARA.txt' ]\n",
    "df = pd.read_csv(\"QuestionnaireData_28Nov2018.csv\", encoding='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    print('Reading file ' + filename + \"...\")\n",
    "    with open(filename, \"r\", encoding='utf8') as textfile:\n",
    "        L = []\n",
    "        for line in textfile:\n",
    "            L.append(line.strip())\n",
    "        print('File contains ', len(L), \"lines.\\n\")\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./sentiment_data_TUN_pos.txt...\n",
      "File contains  3468 lines.\n",
      "\n",
      "Reading file ./sentiment_data_TUN_neg.txt...\n",
      "File contains  4345 lines.\n",
      "\n",
      "Reading file ./langid_data_ARA.txt...\n",
      "File contains  21787 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ara_corpus_pos = read_text_file(corpus_files[0])\n",
    "ara_corpus_neg = read_text_file(corpus_files[1])\n",
    "ara_ara_corpus = read_text_file(corpus_files[2])\n",
    "other_corpus = df.iloc[:,8].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine pos and neg corpus into a single corpus for easy manipulation\n",
    "\n",
    "ara_corpus = ara_corpus_pos + ara_corpus_neg\n",
    "ara_corpus_sentiment = len(ara_corpus_pos)*[1] + len(ara_corpus_neg)*[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7813, 7813)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ara_corpus),len(ara_corpus_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing & Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1. Convert string to emoji\n",
    "dict_emot= { ':-)'  : b'\\xf0\\x9f\\x98\\x8a'.decode('utf-8'),\n",
    "              ':)'   : b'\\xf0\\x9f\\x98\\x8a'.decode('utf-8'),\n",
    "             '=)'   : b'\\xf0\\x9f\\x98\\x8a'.decode('utf-8'),  # Smile or happy\n",
    "              ':-D'  : b'\\xf0\\x9f\\x98\\x83'.decode('utf-8'),\n",
    "              ':D'   : b'\\xf0\\x9f\\x98\\x83'.decode('utf-8'),\n",
    "             '=D'   : b'\\xf0\\x9f\\x98\\x83'.decode('utf-8'),  # Big smile\n",
    "             '>:-(' : b'\\xF0\\x9F\\x98\\xA0'.decode('utf-8'),\n",
    "              '>:-o' : b'\\xF0\\x9F\\x98\\xA0'.decode('utf-8')   # Angry face\n",
    "             }\n",
    "def string_to_emoji(string):\n",
    "    emoji_pattern = re.compile(r\"(:\\-?\\))|(:\\-?D)|=D|=\\)|(>:\\-[o\\(])\")  \n",
    "    return emoji_pattern.sub( lambda x: dict_emot[x.group()] , string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def char_is_emoji(character):\n",
    "    return character in emoji.UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ØªØ¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡\n",
      "ØªØ¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡\n",
      "Ù†ÙˆØ± Ø´ÙŠØ¨Ø© ÙÙ†Ø§Ù† ÙŠØ¹Ø¬Ø¨Ù†ÙŠ ØµÙˆØªÙˆ Ø­Ù„ÙˆØ§ Ùˆ Ù…Ø²ÙŠØ§Ù† :)\n",
      "Ù†ÙˆØ± Ø´ÙŠØ¨Ø© ÙÙ†Ø§Ù† ÙŠØ¹Ø¬Ø¨Ù†ÙŠ ØµÙˆØªÙˆ Ø­Ù„ÙˆØ§ Ùˆ Ù…Ø²ÙŠØ§Ù† ðŸ˜Š\n",
      "â™¥â™¥â™¥\n",
      "â™¥â™¥â™¥\n"
     ]
    }
   ],
   "source": [
    "##1. Remove useless characters using cleanup_text function from TD2\n",
    "\n",
    "# YOU CAN MODIFY THIS FUNCTION AS NEEDED.\n",
    "# FOR EXAMPLE, REMOVE NUMBERS ...\n",
    "\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "try:\n",
    "    maketrans = ''.maketrans\n",
    "except AttributeError:\n",
    "    # fallback for Python 2\n",
    "    from string import maketrans\n",
    "\n",
    "# regexp for word elongation: matches 3 or more repetitions of a word character.\n",
    "two_plus_letters_RE = re.compile(r\"(\\w)\\1{1,}\", re.DOTALL)\n",
    "three_plus_letters_RE = re.compile(r\"(\\w)\\1{2,}\", re.DOTALL)\n",
    "# regexp for repeated words\n",
    "two_plus_words_RE = re.compile(r\"(\\w+\\s+)\\1{1,}\", re.DOTALL)\n",
    "\n",
    "\n",
    "def cleanup_text(text):\n",
    "    \n",
    "    text =string_to_emoji(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "\n",
    "    # Remove user mentions of the form @username\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    \n",
    "    # Replace special html-encoded characters with their ASCII equivalent, for example: &#39 ==> '\n",
    "    if re.search(\"&#\",text):\n",
    "        text = html.unescape(text)\n",
    "\n",
    "    # Remove special useless characters such as _x000D_\n",
    "    text = re.sub(r'_[xX]000[dD]_', '', text)\n",
    "\n",
    "    # Replace all non-word characters (such as  punctuation, end of line characters, etc.) with a space\n",
    "    text = text.translate(maketrans(\"\",\"\", string.punctuation))\n",
    "\n",
    "    # Remove redundant white spaces\n",
    "    text = text.strip()\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "\n",
    "    # normalize word elongations (characters repeated more than twice)\n",
    "    text = two_plus_letters_RE.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # remove repeated words\n",
    "    text = two_plus_words_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# unit test of this function\n",
    "print(ara_corpus[0])\n",
    "print(cleanup_text(ara_corpus[0]))\n",
    "print(ara_corpus[6])\n",
    "print(cleanup_text(ara_corpus[6]))\n",
    "print(ara_corpus[19])\n",
    "print(cleanup_text(ara_corpus[19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this function to each document in the corpus\n",
    "ara_corpus_clean = []\n",
    "\n",
    "for doc in ara_corpus:\n",
    "    ara_corpus_clean.append(cleanup_text(doc))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ØªØ¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡', 'Ø§Ù…Ø±Ø£Ø© Ø£Ù†ÙŠÙ‚Ø© Ùˆ Ø³Ù„Ø³Ø© ÙÙŠ Ø®Ø·Ø§Ø¨Ù‡Ø§ Ùˆ Ø°ÙƒÙŠØ© Ùˆ Ù…ØªÙˆØ§Ø¶Ø¹Ø©', 'Ø¬Ø¹ÙØ± ÙŠØ­Ø¨ Ø¨Ù„Ø§Ø¯Ùˆ ÙˆÙŠØ¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø®ÙŠØ± ÙŠØ¹Ø§ÙˆÙ† ÙÙŠ Ø§Ù„Ù†Ø§Ø³ ÙˆØ§Ù„Ø­ÙƒÙˆÙ…Ø© Ù„Ø§ Ø¹Ù„Ø§Ù‚Ø© ØªØ³Ø±Ù‚ ÙÙŠ Ø§Ù„Ø´Ø¹Ø¨ ÙˆØªØ´Ø±ÙŠ ÙÙŠ Ø§Ù„ÙƒÙˆØ´ Ù„Ù„Ø¨Ø§Ø¬ÙŠ', 'ÙˆØ§Ù„Ù„Ù‡ Ø¨ÙƒØªÙ†ÙŠ Ø¨Ø±Ø§ÙÙˆÙˆÙˆÙˆÙˆ ÙˆØ§ØµÙ„', 'Ù…Ø­Ù„Ø§Ù‡Ø§', 'ÙˆØ§Ù„Ù„Ù‡ Ø§Ø­Ø³Ù† Ù…Ø§ ØµØ§Ø± ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„Ø­ÙˆØ§Ø± Ø§Ù„ØªÙˆÙ†Ø³ÙŠ Ø§Ù„Ù…Ù†Ø´Ø· Ù‡Ø§Ø°Ø§', 'Ù†ÙˆØ± Ø´ÙŠØ¨Ø© ÙÙ†Ø§Ù† ÙŠØ¹Ø¬Ø¨Ù†ÙŠ ØµÙˆØªÙˆ Ø­Ù„ÙˆØ§ Ùˆ Ù…Ø²ÙŠØ§Ù† :)', 'Ø§Ø­Ø³Ù† Ù…Ù…Ø«Ù„ ÙÙŠ Ø§Ø¯ÙˆØ§Ø± Ø§Ù„ÙƒÙˆÙ„', 'Ø§Ù„Ø±Ø¬Ù„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ù†Ø§Ø³Ø¨', 'Ø¹Ø³Ù„']\n",
      "['ØªØ¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡', 'Ø§Ù…Ø±Ø£Ø© Ø£Ù†ÙŠÙ‚Ø© Ùˆ Ø³Ù„Ø³Ø© ÙÙŠ Ø®Ø·Ø§Ø¨Ù‡Ø§ Ùˆ Ø°ÙƒÙŠØ© Ùˆ Ù…ØªÙˆØ§Ø¶Ø¹Ø©', 'Ø¬Ø¹ÙØ± ÙŠØ­Ø¨ Ø¨Ù„Ø§Ø¯Ùˆ ÙˆÙŠØ¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø®ÙŠØ± ÙŠØ¹Ø§ÙˆÙ† ÙÙŠ Ø§Ù„Ù†Ø§Ø³ ÙˆØ§Ù„Ø­ÙƒÙˆÙ…Ø© Ù„Ø§ Ø¹Ù„Ø§Ù‚Ø© ØªØ³Ø±Ù‚ ÙÙŠ Ø§Ù„Ø´Ø¹Ø¨ ÙˆØªØ´Ø±ÙŠ ÙÙŠ Ø§Ù„ÙƒÙˆØ´ Ù„Ù„Ø¨Ø§Ø¬ÙŠ', 'ÙˆØ§Ù„Ù„Ù‡ Ø¨ÙƒØªÙ†ÙŠ Ø¨Ø±Ø§ÙÙˆÙˆ ÙˆØ§ØµÙ„', 'Ù…Ø­Ù„Ø§Ù‡Ø§', 'ÙˆØ§Ù„Ù„Ù‡ Ø§Ø­Ø³Ù† Ù…Ø§ ØµØ§Ø± ÙÙŠ Ù‚Ù†Ø§Ø© Ø§Ù„Ø­ÙˆØ§Ø± Ø§Ù„ØªÙˆÙ†Ø³ÙŠ Ø§Ù„Ù…Ù†Ø´Ø· Ù‡Ø§Ø°Ø§', 'Ù†ÙˆØ± Ø´ÙŠØ¨Ø© ÙÙ†Ø§Ù† ÙŠØ¹Ø¬Ø¨Ù†ÙŠ ØµÙˆØªÙˆ Ø­Ù„ÙˆØ§ Ùˆ Ù…Ø²ÙŠØ§Ù† ðŸ˜Š', 'Ø§Ø­Ø³Ù† Ù…Ù…Ø«Ù„ ÙÙŠ Ø§Ø¯ÙˆØ§Ø± Ø§Ù„ÙƒÙˆÙ„', 'Ø§Ù„Ø±Ø¬Ù„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ù†Ø§Ø³Ø¨', 'Ø¹Ø³Ù„']\n"
     ]
    }
   ],
   "source": [
    "assert(len(ara_corpus_clean)==len(ara_corpus))\n",
    "print(ara_corpus[:10])\n",
    "print(ara_corpus_clean[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LAT_FRAC = 0.3\n",
    "ara_corpus_clean = [doc for doc in ara_corpus_clean if(len(doc) >0) if (len(re.findall('[a-zA-Z]',doc)) / len(doc)) < MAX_LAT_FRAC]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7813 7786\n"
     ]
    }
   ],
   "source": [
    "assert(len(ara_corpus_clean)<=len(ara_corpus))\n",
    "print(len(ara_corpus),len(ara_corpus_clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeArabic(corpus):\n",
    "    corpus = re.sub(\"Ø©\", \"Øª\", corpus)\n",
    "    corpus = re.sub(\"Ø¶\", \"Ø¸\", corpus) \n",
    "    corpus = re.sub(\"Ù‰\", \"ÙŠ\", corpus)\n",
    "    corpus = re.sub(\"Ø¤\", \"Ø¡\", corpus)\n",
    "    corpus = re.sub(\"Ø¦\", \"Ø¡\", corpus)\n",
    "    corpus = re.sub(\"[Ø¥Ø£Ù±Ø¢Ø§]\", \"Ø§\", corpus)\n",
    "    return(corpus)\n",
    "\n",
    "ara_corpus_clean =  [normalizeArabic(doc) for doc in ara_corpus_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[^_\\s]+')\n",
    "ara_corpus_tokenized = [tokenizer.tokenize(doc) for doc in ara_corpus_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert(len(ara_corpus_clean) == len(ara_corpus_tokenized))\n",
    "assert(type(ara_corpus_tokenized[0]) == list and type(ara_corpus_tokenized[0][0]) ==str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ØªØ¨Ø§Ø±Ùƒ', 'Ø§Ù„Ù„Ù‡', 'Ø¹Ù„ÙŠÙ‡'],\n",
       " ['Ø§Ù…Ø±Ø§Øª', 'Ø§Ù†ÙŠÙ‚Øª', 'Ùˆ', 'Ø³Ù„Ø³Øª', 'ÙÙŠ', 'Ø®Ø·Ø§Ø¨Ù‡Ø§', 'Ùˆ', 'Ø°ÙƒÙŠØª', 'Ùˆ', 'Ù…ØªÙˆØ§Ø¸Ø¹Øª'],\n",
       " ['Ø¬Ø¹ÙØ±',\n",
       "  'ÙŠØ­Ø¨',\n",
       "  'Ø¨Ù„Ø§Ø¯Ùˆ',\n",
       "  'ÙˆÙŠØ¹Ù…Ù„',\n",
       "  'ÙÙŠ',\n",
       "  'Ø§Ù„Ø®ÙŠØ±',\n",
       "  'ÙŠØ¹Ø§ÙˆÙ†',\n",
       "  'ÙÙŠ',\n",
       "  'Ø§Ù„Ù†Ø§Ø³',\n",
       "  'ÙˆØ§Ù„Ø­ÙƒÙˆÙ…Øª',\n",
       "  'Ù„Ø§',\n",
       "  'Ø¹Ù„Ø§Ù‚Øª',\n",
       "  'ØªØ³Ø±Ù‚',\n",
       "  'ÙÙŠ',\n",
       "  'Ø§Ù„Ø´Ø¹Ø¨',\n",
       "  'ÙˆØªØ´Ø±ÙŠ',\n",
       "  'ÙÙŠ',\n",
       "  'Ø§Ù„ÙƒÙˆØ´',\n",
       "  'Ù„Ù„Ø¨Ø§Ø¬ÙŠ'],\n",
       " ['ÙˆØ§Ù„Ù„Ù‡', 'Ø¨ÙƒØªÙ†ÙŠ', 'Ø¨Ø±Ø§ÙÙˆÙˆ', 'ÙˆØ§ØµÙ„'],\n",
       " ['Ù…Ø­Ù„Ø§Ù‡Ø§']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify first document in corpus\n",
    "ara_corpus_tokenized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nasri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##5. Remove stop words -- based on a 'standard' list of stopwords for the Arabic language.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Load stop words from NLTK library\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_ar = stopwords.words('arabic')\n",
    "type(stop_words_ar),len(stop_words_ar)\n",
    "set(stop_words_ar) & {'Ù…Ù†','Ø¥Ù„Ù‰','Ø¹Ù†','Ø¹Ù„Ù‰','ÙÙŠ','Ø¨','Ù„','Ùƒ','Ùˆ'}\n",
    "stop_words_ar = stop_words_ar + ['Ù‡Ø¯Ø§','Ù‡Ø§','Ø¨Ø´','Ù…Ù†','Ø¥Ù„Ù‰','Ø¹Ù†','Ø¹Ù„Ù‰','ÙÙŠ','Ø¨','Ù„','Ùƒ','Ùˆ']\n",
    "type(stop_words_ar)\n",
    "# For each document, remove stop words\n",
    "ara_corpus_tokenized = [[word for word in doc if word not in stop_words_ar] for doc in ara_corpus_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##6. Stemming\n",
    "import argparse\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "def light_stem(words):\n",
    "    \n",
    "    result = list()\n",
    "    stemmer = ISRIStemmer()\n",
    "    for word in words:\n",
    "        word = stemmer.norm(word, num=1)      # remove diacritics which representing Arabic short vowels\n",
    "        if not word in stemmer.stop_words:    # exclude stop words from being processed\n",
    "            word = stemmer.pre32(word)        # remove length three and length two prefixes in this order\n",
    "            word = stemmer.suf32(word)        # remove length three and length two suffixes in this order\n",
    "            word = stemmer.waw(word)          # remove connective â€˜Ùˆâ€™ if it precedes a word beginning with â€˜Ùˆâ€™\n",
    "            word = stemmer.norm(word, num=2)  # normalize initial hamza to bare alif\n",
    "        result.append(word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "act=ara_corpus_tokenized\n",
    "ara_corpus_clean =  [light_stem(words) for words in ara_corpus_tokenized ]\n",
    "ara_corpus_tokenized =  [tokenizer.tokenize(doc) for doc in ara_corpus_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ØªØ¨Ø§Ø±Ùƒ', 'Ø§Ù„Ù„Ù‡'], ['Ø§Ù…Ø±Ø§Øª', 'Ø§Ù†ÙŠÙ‚Øª', 'Ø³Ù„Ø³Øª', 'Ø®Ø·Ø§Ø¨Ù‡Ø§', 'Ø°ÙƒÙŠØª', 'Ù…ØªÙˆØ§Ø¸Ø¹Øª'], ['Ø¬Ø¹ÙØ±', 'ÙŠØ­Ø¨', 'Ø¨Ù„Ø§Ø¯Ùˆ', 'ÙˆÙŠØ¹Ù…Ù„', 'Ø§Ù„Ø®ÙŠØ±', 'ÙŠØ¹Ø§ÙˆÙ†', 'Ø§Ù„Ù†Ø§Ø³', 'ÙˆØ§Ù„Ø­ÙƒÙˆÙ…Øª', 'Ø¹Ù„Ø§Ù‚Øª', 'ØªØ³Ø±Ù‚', 'Ø§Ù„Ø´Ø¹Ø¨', 'ÙˆØªØ´Ø±ÙŠ', 'Ø§Ù„ÙƒÙˆØ´', 'Ù„Ù„Ø¨Ø§Ø¬ÙŠ'], ['ÙˆØ§Ù„Ù„Ù‡', 'Ø¨ÙƒØªÙ†ÙŠ', 'Ø¨Ø±Ø§ÙÙˆÙˆ', 'ÙˆØ§ØµÙ„'], ['Ù…Ø­Ù„Ø§Ù‡Ø§'], ['ÙˆØ§Ù„Ù„Ù‡', 'Ø§Ø­Ø³Ù†', 'ØµØ§Ø±', 'Ù‚Ù†Ø§Øª', 'Ø§Ù„Ø­ÙˆØ§Ø±', 'Ø§Ù„ØªÙˆÙ†Ø³ÙŠ', 'Ø§Ù„Ù…Ù†Ø´Ø·', 'Ù‡Ø§Ø°Ø§']]\n",
      "[['ØªØ¨Ø§Ø±Ùƒ', 'Ø§Ù„Ù„Ù‡'], ['Ø§Ù…Ø±', 'Ø§Ù†ÙŠÙ‚Øª', 'Ø³Ù„Ø³Øª', 'Ø®Ø·Ø§Ø¨', 'Ø°ÙƒÙŠØª', 'Ù…ØªÙˆØ§Ø¸Ø¹Øª'], ['Ø¬Ø¹ÙØ±', 'ÙŠØ­Ø¨', 'Ø¨Ù„Ø§Ø¯Ùˆ', 'ÙˆÙŠØ¹Ù…Ù„', 'Ø®ÙŠØ±', 'ÙŠØ¹Ø§', 'Ù†Ø§Ø³', 'Ø­ÙƒÙˆÙ…Øª', 'Ø¹Ù„Ø§Ù‚Øª', 'ØªØ³Ø±Ù‚', 'Ø´Ø¹Ø¨', 'ÙˆØªØ´Ø±ÙŠ', 'ÙƒÙˆØ´', 'Ø¨Ø§Ø¬ÙŠ'], ['ÙˆØ§Ù„Ù„Ù‡', 'Ø¨ÙƒØª', 'Ø¨Ø±Ø§ÙÙˆÙˆ', 'ÙˆØ§ØµÙ„'], ['Ù…Ø­Ù„Ø§'], ['ÙˆØ§Ù„Ù„Ù‡', 'Ø§Ø­Ø³Ù†', 'ØµØ§Ø±', 'Ù‚Ù†Ø§Øª', 'Ø­ÙˆØ§Ø±', 'ØªÙˆÙ†Ø³ÙŠ', 'Ù…Ù†Ø´Ø·', 'Ù‡Ø§Ø°Ø§']]\n"
     ]
    }
   ],
   "source": [
    "print(act[:6])\n",
    "print(ara_corpus_tokenized[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14243.000000\n",
       "mean         4.882398\n",
       "std          1.852973\n",
       "min          1.000000\n",
       "25%          4.000000\n",
       "50%          5.000000\n",
       "75%          6.000000\n",
       "max         58.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##7. Remove words that are too short or too long.\n",
    "\n",
    "distinct_words = {word for doc in ara_corpus_tokenized for word in doc}\n",
    "type(distinct_words),len(distinct_words)\n",
    "\n",
    "words_len = pd.Series([len(word) for word in distinct_words])\n",
    "words_len.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 7786)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ara_corpus_tokenized = [[word for word in doc if len(word)>=4 or char_is_emoji(word)] for doc in ara_corpus_tokenized]\n",
    "ara_corpus_tokenized = [[word for word in doc if len(word)<=12] for doc in ara_corpus_tokenized]\n",
    "type(ara_corpus_tokenized),len(ara_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ØªØ¨Ø§Ø±Ùƒ', 'Ø§Ù„Ù„Ù‡'], ['Ø§Ù†ÙŠÙ‚Øª', 'Ø³Ù„Ø³Øª', 'Ø®Ø·Ø§Ø¨', 'Ø°ÙƒÙŠØª', 'Ù…ØªÙˆØ§Ø¸Ø¹Øª'], ['Ø¬Ø¹ÙØ±', 'Ø¨Ù„Ø§Ø¯Ùˆ', 'ÙˆÙŠØ¹Ù…Ù„', 'Ø­ÙƒÙˆÙ…Øª', 'Ø¹Ù„Ø§Ù‚Øª', 'ØªØ³Ø±Ù‚', 'ÙˆØªØ´Ø±ÙŠ', 'Ø¨Ø§Ø¬ÙŠ'], ['ÙˆØ§Ù„Ù„Ù‡', 'Ø¨Ø±Ø§ÙÙˆÙˆ', 'ÙˆØ§ØµÙ„'], ['Ù…Ø­Ù„Ø§'], ['ÙˆØ§Ù„Ù„Ù‡', 'Ø§Ø­Ø³Ù†', 'Ù‚Ù†Ø§Øª', 'Ø­ÙˆØ§Ø±', 'ØªÙˆÙ†Ø³ÙŠ', 'Ù…Ù†Ø´Ø·', 'Ù‡Ø§Ø°Ø§'], ['Ø´ÙŠØ¨Øª', 'ÙÙ†Ø§Ù†', 'ÙŠØ¹Ø¬Ø¨', 'ØµÙˆØªÙˆ', 'Ø­Ù„ÙˆØ§', 'ðŸ˜Š'], ['Ø§Ø­Ø³Ù†', 'Ù…Ù…Ø«Ù„', 'Ø§Ø¯ÙˆØ§Ø±'], ['Ù…Ù†Ø§Ø³Ø¨', 'Ù…ÙƒØ§Ù†', 'Ù…Ù†Ø§Ø³Ø¨'], []]\n"
     ]
    }
   ],
   "source": [
    "print(ara_corpus_tokenized[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a  language classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a language classifier using supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tun_corpus_clean = [cleanup_text(doc)   for doc in ara_corpus]\n",
    "ara_corpus_clean = [cleanup_text(doc)   for doc in ara_ara_corpus]\n",
    "\n",
    "# Step 1   COMPLETE THE CODE BELOW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#?train_test_split\n",
    "\n",
    "tun_corpus_clean_train, tun_corpus_clean_test = train_test_split(tun_corpus_clean,test_size=0.3 )\n",
    "\n",
    "ara_corpus_clean_train, ara_corpus_clean_test = train_test_split(ara_corpus_clean,test_size=0.3 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1   create 2 data frames called train_df and test_df (as explained above)\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "# create data frame\n",
    "train_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "\n",
    "# fill the language column\n",
    "train_df.language = pd.Series(['TUN']*len(tun_corpus_clean_train) + ['ARA']*len(tun_corpus_clean_train) )\n",
    "\n",
    "\n",
    "# fill the document column -- CONCATENATE the TUN CORPUS and ARA CORPUS\n",
    "train_df.document = pd.Series(tun_corpus_clean_train + ara_corpus_clean_train )\n",
    "\n",
    "test_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "test_df.language = pd.Series(['TUN']*len(tun_corpus_clean_test) + ['ARA']*len(ara_corpus_clean_test) )\n",
    "test_df.document = pd.Series(tun_corpus_clean_test + ara_corpus_clean_test   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2  Convert the training documents into numeric feature vectors using the BOW-tfidf method with character ngrams.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "n = 3   # hyperparameter for of character ngrams ; you can change it if you want but n=3 is a reaonable value ...\n",
    "\n",
    "# Create an instance of TfidfVectorizer class with analyzer = 'char' so that it generates bag of characters and not bag of words\n",
    "bow_model_char = TfidfVectorizer(analyzer='char', ngram_range=(1,n), max_df =0.9, min_df=0.1)\n",
    "\n",
    "# Call fit method with the combined training corpus\n",
    "bow_model_char.fit(train_df.document)\n",
    "\n",
    "# Create DTM matrix of the combined training corpus and test corpus\n",
    "dtm_Train=bow_model_char.transform(train_df.document)\n",
    "dtm_Test=bow_model_char.transform(test_df.document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TUN'] TUN\n",
      "Confusion matrix :  [[5997  540]\n",
      " [ 269 2075]]\n"
     ]
    }
   ],
   "source": [
    "# Step 3   -- see official documentation of MultinomialNB in scikit-learn\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(dtm_Train,train_df.language)\n",
    "\n",
    "print(nb_model.predict(dtm_Test[0]),test_df.language[0])\n",
    "\n",
    "print(\"Confusion matrix : \",confusion_matrix(test_df.language,nb_model.predict(dtm_Test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(doc):\n",
    "    dtm=bow_model_char.transform(pd.Series(doc))\n",
    "    return nb_model.predict(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splt(x):\n",
    "    u = str(x).split(':')\n",
    "    return [u[0][1:],float(u[1][0:-2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(doc):\n",
    "    lg=splt(langdetect.detect_langs(doc))\n",
    "    if lg[0]=='ar' :\n",
    "        Lang=predict(doc)\n",
    "    else:\n",
    "        Lang='[\\'OTHER\\']'\n",
    "    print(Lang)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OTHER']\n",
      "['ARA']\n",
      "['TUN']\n"
     ]
    }
   ],
   "source": [
    "lang_detect(other_corpus[0])\n",
    "lang_detect(ara_corpus_clean_test[0])\n",
    "lang_detect(tun_corpus_clean_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the corpus for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, concatenate the words in the cleaned corpus (because BOW method in scikit-learn requires this format)\n",
    "ara_corpus_bow = [' '.join(doc) for doc in ara_corpus_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Configuration parameters of the BOW model\n",
    "# FEEL FREE TO MODIFY THESE PARAMETERS AS NEEDED ...\n",
    "max_words = 10000\n",
    "maxdf = 0.7\n",
    "mindf = 5\n",
    "# create an instance of this class\n",
    "bow_model = TfidfVectorizer(max_df=maxdf, min_df=mindf, max_features=max_words, stop_words=[], use_idf = True)\n",
    "# call fit() method in order to prepare BOW method (determine vocabulary and IDF values)\n",
    "bow_model.fit( ara_corpus_bow )\n",
    "# Call the transform method in order to calculate DTM matrix of our corpus\n",
    "ara_bow_dtm = bow_model.transform(ara_corpus_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove documents that do not contain any vocabulary terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_terms_per_doc = np.array((ara_bow_dtm>0).sum(axis=1))  # calculate sum of rows of DTM matrix\n",
    "nb_terms_per_doc = nb_terms_per_doc.ravel()  # convert result to a 1D array (instead of 2D array)\n",
    "idx = nb_terms_per_doc>0\n",
    "ara_bow_dtm_filt = ara_bow_dtm[nb_terms_per_doc>0,:]\n",
    "ara_corpus_bow_filt = [ara_corpus_bow[i] for i,x in enumerate(idx) if x]\n",
    "ara_corpus_sentiment_filt = [ara_corpus_sentiment[i] for i,x in enumerate(idx) if x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Sentiment Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ara_bow_dtm_filt\n",
    "y = ara_corpus_sentiment_filt\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1076,   83],\n",
       "       [  84,  870]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build Naive Bayes classification model\n",
    "\n",
    "NB_model = MultinomialNB(alpha = 1.0)\n",
    "NB_model.fit(X_train, y_train)\n",
    "# Use this model to predict the sentiment category of test documents\n",
    "y_pred_NB = NB_model.predict(X_test)\n",
    "# Classification rate\n",
    "metrics.accuracy_score(y_test, y_pred_NB)\n",
    "# Confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classefy(doc):\n",
    "    ara_corpus_bow=[]\n",
    "    ara_corpus_bow.append(doc)\n",
    "    ara_bow_dtm = bow_model.transform(ara_corpus_bow)\n",
    "    nb_terms_per_doc = np.array((ara_bow_dtm>0).sum(axis=1))  # calculate sum of rows of DTM matrix\n",
    "    nb_terms_per_doc = nb_terms_per_doc.ravel()  # convert result to a 1D array (instead of 2D array)\n",
    "    idx = nb_terms_per_doc>0\n",
    "    ara_bow_dtm_filt = ara_bow_dtm[nb_terms_per_doc>0,:]\n",
    "    ara_corpus_bow_filt = [ara_corpus_bow[i] for i,x in enumerate(idx) if x]\n",
    "    if len(ara_corpus_bow_filt)==0:\n",
    "        print('0')\n",
    "    else:\n",
    "        y=(NB_model.predict(ara_bow_dtm_filt))\n",
    "        print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "classefy(other_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "classefy(ara_corpus_neg [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "classefy(ara_corpus_pos [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1097   62]\n",
      " [ 117  837]]\n"
     ]
    }
   ],
   "source": [
    "# Train the model using Logistic Regression method\n",
    "LR_model = LogisticRegression(penalty='l2')\n",
    "LR_model.fit(X_train, y_train)\n",
    "# Use this model to predict the sentiment category of test documents\n",
    "y_pred_LR = LR_model.predict(X_test)\n",
    "# Calculate the classification rate of this classifier\n",
    "metrics.accuracy_score(y_test, y_pred_LR)\n",
    "# Display the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB method seems better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
